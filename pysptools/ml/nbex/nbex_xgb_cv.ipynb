{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Cross Validation\n",
    "\n",
    "The Python wrap around XGBoots implements a scikit-learn interface and this interface, more or less, support the scikit-learn cross validation system. More, XGBoost have is own cross validation system and the Python wrap support it. In other words, we have two cross validation systems. They are partialy supported and the functionalities supported for XGBoost are not the same for LightGBM. Currently, it's a puzzle.\n",
    "\n",
    "The example presented covers both cases. The first, step_GradientBoostingCV, call the XGBoost cross validation. The second, step_GridSearchCV, call the scikit-learn cross validation.\n",
    "\n",
    "The data preparation is the same as for the nbex_xgb_model.ipynb example. We take only two images to speed up the process.\n",
    "\n",
    "The 'Tune' class manages everything.\n",
    "The step_GradientBoostingCV method call the XGBoost cv() function.\n",
    "The step_GridSearchCV method call the scikit-learn GridSearchCV() function.\n",
    "\n",
    "Take note that this is in development and that changes can be significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import pysptools.ml as ml\n",
    "import pysptools.skl as skl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "home_path = os.environ['HOME']\n",
    "source_path = osp.join(home_path, 'dev-data/CZ_hsdb')\n",
    "result_path = None\n",
    "\n",
    "\n",
    "def print_step_header(step_id, title):\n",
    "    print('================================================================')\n",
    "    print('{}: {}'.format(step_id, title))\n",
    "    print('================================================================')\n",
    "    print()\n",
    "\n",
    "\n",
    "# img1\n",
    "img1_scaled, img1_cmap = ml.get_scaled_img_and_class_map(source_path, result_path, 'img1', \n",
    "                          [['Snow',{'rec':(41,79,49,100)}]],\n",
    "                          skl.HyperGaussianNB, None,\n",
    "                          display=False)\n",
    "# img2\n",
    "img2_scaled, img2_cmap = ml.get_scaled_img_and_class_map(source_path, result_path, 'img2', \n",
    "                          [['Snow',{'rec':(83,50,100,79)},{'rec':(107,151,111,164)}]],\n",
    "                          skl.HyperLogisticRegression, {'class_weight':{0:1.0,1:5}},\n",
    "                          display=False)\n",
    "\n",
    "\n",
    "def step_GradientBoostingCV(tune, update, cv_params, verbose):\n",
    "    print_step_header('Step', 'GradientBoosting cross validation')\n",
    "    tune.print_params('input')\n",
    "    tune.step_GradientBoostingCV(update, cv_params, verbose)\n",
    "\n",
    "\n",
    "def step_GridSearchCV(tune, params, title, verbose):\n",
    "    print_step_header('Step', 'scikit-learn cross-validation')\n",
    "    tune.print_params('input')\n",
    "    tune.step_GridSearchCV(params, title, verbose)\n",
    "    tune.print_params('output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train and y_train sets are built\n",
    "\n",
    "The class Tune is created with the HyperXGBClassifier estimator. It's ready for cross validation, we can call Tune methods repeatedly with differents cv hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = False\n",
    "n_shrink = 3\n",
    "\n",
    "snow_fname = ['img1','img2']\n",
    "nosnow_fname = ['imga1','imgb1','imgb6','imga7']\n",
    "all_fname = snow_fname + nosnow_fname\n",
    "\n",
    "snow_img = [img1_scaled,img2_scaled]\n",
    "nosnow_img = ml.batch_load(source_path, nosnow_fname, n_shrink)\n",
    "\n",
    "snow_cmap = [img1_cmap,img2_cmap]\n",
    "\n",
    "M = snow_img[0]\n",
    "bkg_cmap = np.zeros((M.shape[0],M.shape[1]))\n",
    "    \n",
    "X,y = skl.shape_to_XY(snow_img+nosnow_img, \n",
    "                      snow_cmap+[bkg_cmap,bkg_cmap,bkg_cmap,bkg_cmap])\n",
    "\n",
    "seed = 5\n",
    "train_size = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size,\n",
    "                                                    random_state=seed)\n",
    "\n",
    "start_param = {'max_depth':10,\n",
    "               'min_child_weight':1,\n",
    "               'gamma':0,\n",
    "               'subsample':0.8,\n",
    "               'colsample_bytree':0.5,\n",
    "               'scale_pos_weight':1.5}\n",
    "      \n",
    "# Tune can be call with HyperXGBClassifier or HyperLGBMClassifier,\n",
    "# but hyperparameters and cv parameters are differents\n",
    "t = ml.Tune(ml.HyperXGBClassifier, start_param, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set an hypothesis and call the Gradient Boosting cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Step: GradientBoosting cross validation\n",
      "================================================================\n",
      "\n",
      "----------------------------------------------------------------\n",
      "input parameters:\n",
      "\n",
      "parameter           value\n",
      "----------------  -------\n",
      "colsample_bytree      0.5\n",
      "gamma                 0\n",
      "max_depth            10\n",
      "min_child_weight      1\n",
      "scale_pos_weight      1.5\n",
      "subsample             0.8\n",
      "\n",
      "----------------------------------------------------------------\n",
      "XGBoost cross-validation tail\n",
      "\n",
      "   test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std\n",
      "5        0.149833       0.002526         0.138030        0.000989\n",
      "6        0.128094       0.003152         0.112691        0.001023\n",
      "7        0.111777       0.003751         0.092579        0.000973\n",
      "8        0.100013       0.004289         0.076613        0.001092\n",
      "9        0.091255       0.004772         0.063826        0.001418\n",
      "\n",
      "----------------------------------------------------------------\n",
      "output parameters:\n",
      "\n",
      "parameter           value\n",
      "----------------  -------\n",
      "colsample_bytree      0.5\n",
      "gamma                 0\n",
      "learning_rate         0.2\n",
      "max_depth            10\n",
      "min_child_weight      1\n",
      "n_estimators          9\n",
      "scale_pos_weight      1.5\n",
      "silent                1\n",
      "subsample             0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Fix learning rate and number of estimators for tuning tree-based parameters\n",
    "step_GradientBoostingCV(t, {'learning_rate':0.2,'n_estimators':500,'silent':1},\n",
    "                        {'verbose_eval':False},\n",
    "                        True)\n",
    "# After reading the cross validation results we manually set n_estimator\n",
    "t.p_update({'n_estimators':9})\n",
    "t.print_params('output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same but this time we call the scikit-learn cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Step: scikit-learn cross-validation\n",
      "================================================================\n",
      "\n",
      "----------------------------------------------------------------\n",
      "input parameters:\n",
      "\n",
      "parameter           value\n",
      "----------------  -------\n",
      "colsample_bytree      0.5\n",
      "gamma                 0\n",
      "learning_rate         0.2\n",
      "max_depth            10\n",
      "min_child_weight      1\n",
      "n_estimators          9\n",
      "scale_pos_weight      1.5\n",
      "silent                1\n",
      "subsample             0.8\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Cross validation inputs:\n",
      "\n",
      "n splits: 2\n",
      "Shuffle: True\n",
      "\n",
      "Parameters grid:\n",
      "\n",
      "----------------  ------------\n",
      "max_depth         [24, 25, 26]\n",
      "min_child_weight  [1]\n",
      "----------------  ------------\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Cross validation results:\n",
      "\n",
      "Best score: 0.992160330091\n",
      "\n",
      "Best parameters:\n",
      "\n",
      "----------------  --\n",
      "max_depth         24\n",
      "min_child_weight   1\n",
      "----------------  --\n",
      "\n",
      "All scores:\n",
      "\n",
      "  max_depth    min_child_weight    score         std\n",
      "-----------  ------------------  -------  ----------\n",
      "         24                   1  0.99216  0.00011789\n",
      "         25                   1  0.99216  0.00011789\n",
      "         26                   1  0.99216  0.00011789\n",
      "\n",
      "----------------------------------------------------------------\n",
      "output parameters:\n",
      "\n",
      "parameter           value\n",
      "----------------  -------\n",
      "colsample_bytree      0.5\n",
      "gamma                 0\n",
      "learning_rate         0.2\n",
      "max_depth            24\n",
      "min_child_weight      1\n",
      "n_estimators          9\n",
      "scale_pos_weight      1.5\n",
      "silent                1\n",
      "subsample             0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tune max_depth and min_child_weight\n",
    "step_GridSearchCV(t, {'max_depth':[24,25, 26], 'min_child_weight':[1]}, 'Step 2', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0, 'subsample': 0.8, 'colsample_bytree': 0.5, 'scale_pos_weight': 1.5, 'learning_rate': 0.2, 'n_estimators': 9, 'silent': 1, 'max_depth': 24, 'min_child_weight': 1}\n"
     ]
    }
   ],
   "source": [
    "print(t.get_p_current())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
